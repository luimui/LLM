{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d4ebc988-0863-4158-a299-155421019a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"I'm going to train my dog to sit on the train,\" he said.\n",
      "\n",
      "\"I'm going to train my dog to sit on the train,\" he said.\n",
      "\n",
      "\"I'm going to train my dog to sit on the train,\" he said.\n",
      "\n",
      "\"I'm going to train my dog to sit on the train,\" he said.\n",
      "\n",
      "\"I'm going to train my dog to sit on the train,\" he said.\n",
      "\n",
      "\"I'm going to train my dog to sit on the train,\" he said.\n",
      "\n",
      "\"I'm going to train my dog to sit on the train,\" he said.\n",
      "\n",
      "\"I'm going to train my dog to sit on the train,\" he said.\n",
      "\n",
      "\"I'm going to train my dog to sit on the train,\" he said.\n",
      "\n",
      "\"I'm going to train my dog to sit on the train,\" he said.\n",
      "\n",
      "\"I'm going to train my dog to sit on the train,\" he said.\n",
      "\n",
      "\"I'm going to train my dog to sit on the train,\" he said.\n",
      "\n",
      "\"I'm going to train my dog to sit on the train,\" he said.\n",
      "\n",
      "\"I'm going to train my\n"
     ]
    }
   ],
   "source": [
    "from llama import BasicModelRunner\n",
    "import itertools\n",
    "from html2text import HTML2Text\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "model = \"EleutherAI/pythia-70m\"\n",
    "non_finetuned = BasicModelRunner(model)\n",
    "non_finetunde_output = non_finetuned(\"Tell ne how to train my dog to sit.\")\n",
    "print(non_finetunde_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c9ad8ba7-1056-4d21-8d65-8b53e532b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"ymoslem/Law-StackExchange\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c41220aa-7231-4983-bfa7-5179e2dca7f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['answers', 'license', 'tags', 'question_body', 'score', 'question_title', 'question_id', 'link'],\n",
       "    num_rows: 24370\n",
       "})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c557ad27-d578-4181-907c-3df6ae962420",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_q = \"\"\"### Question:{question} ### Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ba701-1773-405e-afb1-2a61dc311213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "792959b5-13ee-4a0f-90a0-f43bf6963f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = HTML2Text()\n",
    "h.body_width = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "307249b3-249c-49b3-aa88-a74d90362d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "35915b5a-0814-4e0c-93b1-c76559efc3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d56ea903-3ad5-4c04-a89d-be98a148a906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Law-Stackexchange dataset:\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "dataList = []\n",
    "print(\"Law-Stackexchange dataset:\")\n",
    "top_n = itertools.islice(dataset, n)\n",
    "for i in top_n:\n",
    "    question = i[\"question_body\"]\n",
    "    question = h.handle(i[\"question_body\"])\n",
    "    question = question.replace('\\n', '')\n",
    "    question = question.replace('###', '')\n",
    "   \n",
    "    text_with_prompt_template_q = prompt_template_q.format(question=question)\n",
    "    \n",
    "    \n",
    "    answer = h.handle(i[\"answers\"][0][\"body\"])\n",
    "    answer = answer.replace('\\n', '')\n",
    "    answer = answer.replace('###', '')\n",
    "\n",
    "    tokenized_inputs = tokenize_function(question+answer)\n",
    "    \n",
    "\n",
    "   \n",
    "    dataList.append({\"question\": text_with_prompt_template_q, \"answer\": answer, \"input_ids\": tokenized_inputs[\"input_ids\"],\"attention_mask\":tokenized_inputs[\"attention_mask\"]}) \n",
    "  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ee735f33-ad1d-4822-b5c4-41e0838c7091",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "('Can only convert 1-dimensional array values', 'Conversion failed for column input_ids with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataList\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:860\u001b[0m, in \u001b[0;36mDataset.from_pandas\u001b[0;34m(cls, df, features, info, split, preserve_index)\u001b[0m\n\u001b[1;32m    858\u001b[0m     info \u001b[38;5;241m=\u001b[39m DatasetInfo()\n\u001b[1;32m    859\u001b[0m info\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m features\n\u001b[0;32m--> 860\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mInMemoryTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;66;03m# more expensive cast than InMemoryTable.from_pandas(..., schema=features.arrow_schema)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;66;03m# needed to support the str to Audio conversion for instance\u001b[39;00m\n\u001b[1;32m    867\u001b[0m     table \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mcast(features\u001b[38;5;241m.\u001b[39marrow_schema)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/table.py:761\u001b[0m, in \u001b[0;36mInMemoryTable.from_pandas\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pandas\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    707\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    Convert pandas.DataFrame to an Arrow Table.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyarrow/table.pxi:3869\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyarrow/pandas_compat.py:613\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    609\u001b[0m             arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    610\u001b[0m             \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 613\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [convert_column(c, f)\n\u001b[1;32m    614\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m c, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(columns_to_convert, convert_fields)]\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyarrow/pandas_compat.py:613\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    609\u001b[0m             arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mcontiguous \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    610\u001b[0m             \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 613\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\u001b[43mconvert_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m c, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(columns_to_convert, convert_fields)]\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyarrow/pandas_compat.py:600\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    596\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    597\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    598\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    599\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field_nullable \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mnull_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m was non-nullable but pandas column \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m null values\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(field),\n\u001b[1;32m    604\u001b[0m                                                  result\u001b[38;5;241m.\u001b[39mnull_count))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyarrow/pandas_compat.py:594\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    591\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 594\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    596\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    597\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    598\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    599\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyarrow/array.pxi:340\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyarrow/array.pxi:86\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: ('Can only convert 1-dimensional array values', 'Conversion failed for column input_ids with type object')"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=dataList))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b094a03e-1f96-4216-aa83-e6bd3d2b8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(\"Hi how are you?\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b68c7dd9-0d94-4ce3-aac3-62ee3137ee68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c540d644-5a94-47c8-91e6-d360068b5c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "262e99c8-7748-484f-9823-5604c8695381",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m(\n\u001b[1;32m      2\u001b[0m     tokenize_function,\n\u001b[1;32m      3\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m     drop_last_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_dataset)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenized_dataset = dataList.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
